#!/usr/bin/env python
'''
viscRNA-Seq pipeline local script.

This script runs on the newly allocated EC2 instance as directed by the viscrna-seq-pipeline script and does the heavy lifting.
'''
import os
import sys
import shutil
import subprocess as sp
import argparse


class Pipeline:
    def __init__(
            self,
            input_data_path,
            input_samplesheet_path,
            output_path,
            output_gdrive_path,
            dataset_id,
            samplenames,
            chemistry,
            expect_cells,
            ):

        self.input_data_path = input_data_path
        self.input_samplesheet_path = input_samplesheet_path
        self.output_path = output_path
        self.output_gdrive_path = output_gdrive_path
        self.dataset_id = dataset_id
        self.samplenames = samplenames
        self.chemistry = chemistry
        self.expect_cells = expect_cells

    def remove_old_data(self):
        shutil.rmtree('/data/{:}'.format(self.dataset_id), ignore_errors=True)
        shutil.rmtree('/home/ubuntu/{:}'.format(self.dataset_id), ignore_errors=True)
        shutil.rmtree('/home/ubuntu/__{:}.mro'.format(self.dataset_id), ignore_errors=True)

    def make_data_folders(self):
        os.makedirs('/data/{:}'.format(self.dataset_id), exist_ok=True)
        os.makedirs('/data/{:}/bcl'.format(self.dataset_id), exist_ok=True)
        os.makedirs('/data/{:}/log'.format(self.dataset_id), exist_ok=True)
        os.makedirs('/data/{:}/count'.format(self.dataset_id), exist_ok=True)

    def get_data(self, path):
        if path.startswith('s3://'):
            return self.get_data_s3(path)
        elif path.startswith('http://') or path.startswith('https://'):
            return self.get_data_url(path)
        else:
            raise ValueError('Type of data source not recognized: {:}'.format(path))

    def unpack_data(self, path):
        parent = '/data/{:}/bcl'.format(self.dataset_id)
        fn = '/data/{:}/bcl/{:}'.format(self.dataset_id, os.path.basename(path))

        if fn.endswith('.tar.gz') or fn.endswith('.tar.xz'):
            sp.run(' '.join([
                'tar', '-xf', path, '-C', parent,
                ]),
                shell=True,
                check=True)
        else:
            raise ValueError('Compressed format not supported: {:}'.format(
                fn.split('.')[-1]))

        os.remove(path)

    def get_data_s3(self, url):
        fn = '/data/{:}/bcl/{:}'.format(self.dataset_id, url.split('/')[-1])
        sp.run(' '.join([
            'aws', 's3', 'cp',
            url, fn,
            ]),
            shell=True,
            check=True,
            )

    def get_data_url(self, url):
        os.chdir('/data/{:}/bcl'.format(self.dataset_id))
        try:
            sp.run(' '.join([
                'wget', url,
                ]),
                shell=True,
                check=True,
                )
        finally:
            os.chdir('/home/ubuntu')

    def copy_output_data_mkfastq(self):
        print('Transfer fastqs')
        sp.run(' '.join([
            'aws', 's3', 'cp',
            '--recursive',
            '/data/{:}/fastq'.format(self.dataset_id),
            's3://viscrna-seq/data/{:}/fastq'.format(self.dataset_id)
            ]),
            shell=True,
            check=True)

        print('Transfer mkfastq logs')
        sp.run(' '.join([
            'aws', 's3', 'cp',
            '--recursive',
            '/data/{:}/log/mkfastq'.format(self.dataset_id),
            's3://viscrna-seq/data/{:}/log/mkfastq'.format(self.dataset_id)
            ]),
            shell=True,
            check=True)

    def copy_output_data_count(self, samplename):
        print('Transfer count tables')
        sp.run(' '.join([
            'aws', 's3', 'cp',
            '--recursive',
            '/data/{:}/count/{:}'.format(self.dataset_id, samplename),
            's3://viscrna-seq/data/{:}/count/{:}'.format(
                self.dataset_id,
                samplename)
            ]),
            shell=True,
            check=True)

        print('Transfer count logs')
        sp.run(' '.join([
            'aws', 's3', 'cp',
            '--recursive',
            '/data/{:}/log/count/{:}'.format(self.dataset_id, samplename),
            's3://viscrna-seq/data/{:}/log/count/{:}'.format(
                self.dataset_id, samplename)
            ]),
            shell=True,
            check=True)

        print('Transfer count table to GDrive')
        print('TODO')
        # TODO

    def __call__(self):
        print('RUN PIPELINE')
        print('Remove old data')
        self.remove_old_data()

        print('Make data folders')
        self.make_data_folders()

        print('Get data')
        self.get_data(self.input_data_path)
        self.input_data_path = '/data/{:}/bcl/{:}'.format(
                self.dataset_id,
                os.path.basename(self.input_data_path))

        print('Unpack data')
        self.unpack_data(self.input_data_path)
        self.input_data_path = '.'.join(self.input_data_path.split('.')[:-2])

        print('Get samplesheet')
        self.get_data(self.input_samplesheet_path)
        self.input_samplesheet_path = '/data/{:}/bcl/{:}'.format(
                self.dataset_id,
                os.path.basename(self.input_samplesheet_path))

        print('Make fastq')
        os.makedirs('/data/fastq', exist_ok=True)
        sp.run(' '.join([
            'cellranger',
            'mkfastq',
            '--id={:}'.format(self.dataset_id),
            '--run={:}'.format(self.input_data_path),
            '--samplesheet={:}'.format(self.input_samplesheet_path),
            '--output-dir=/data/{:}/fastq'.format(self.dataset_id),
            ]),
            shell=True,
            check=True)
        shutil.copytree(
                self.dataset_id,
                '/data/{:}/log/mkfastq'.format(self.dataset_id),
                )
        shutil.rmtree(
                self.dataset_id,
                ignore_errors=True)
        self.copy_output_data_mkfastq()

        for samplename in self.samplenames:
            print('Count genes for sample: {:}'.format(samplename))
            sp.run(' '.join([
                'cellranger',
                'count',
                '--id='+self.dataset_id,
                '--transcriptome=/assets/references/mouse_and_mCMV',
                '--fastqs=/data/{:}/fastq'.format(self.dataset_id),
                '--sample={:}'.format(samplename),
                '--expect-cells={:}'.format(self.expect_cells),
                '--chemistry={:}'.format(self.chemistry),
                '--nosecondary',
                ]),
                shell=True,
                check=True)
            # NOTE: output goes in a subfolder of the cwd called like the sample:
            # $HOME/<self.dataset_id>/outs/possorted_genome_bam.bam
            # $HOME/<self.dataset_id>/outs/filtered_gene_bc_matrices_h5.h5
            # see: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/overview
            shutil.copytree(
                    self.dataset_id+'/outs',
                    '/data/{:}/count/{:}'.format(
                        self.dataset_id,
                        samplename))
            shutil.rmtree(
                    self.dataset_id+'/outs',
                    ignore_errors=True)
            shutil.copytree(
                    self.dataset_id,
                    '/data/{:}/log/count/{:}'.format(
                        self.dataset_id,
                        samplename))
            shutil.rmtree(
                    self.dataset_id,
                    ignore_errors=True)
            self.copy_output_data_counts(samplename)


        print('Done')



if __name__ == '__main__':

    pa = argparse.ArgumentParser(description='''viscRNA-Seq pipeline''')
    pa.add_argument(
            '--input-data', required=True,
            help='The location of the raw reads')
    pa.add_argument(
            '--input-samplesheet', required=True,
            help='The location of the samplesheet')
    pa.add_argument(
            '--output', required=True,
            help='The location of the output S3 bucket')
    pa.add_argument(
            '--output-gdrive', required=False, default=None,
            help='The location on GoogleDrive where to store the count table')
    pa.add_argument(
            '--id', required=True,
            help='Sets a unique ID for the dataset, for record keeping')
    pa.add_argument(
            '--samplenames', nargs='+', required=True,
            help='Samplenames in the samplesheet')
    pa.add_argument(
            '--chemistry', required=False, default='threeprime',
            choices=[
                'auto', 'threeprime', 'fiveprime',
                'SC3Pv1', 'SC3Pv2', 'SC5P-PE', 'SC5P-R2',
                ],
            help='Chemistry used in the data, see https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/count')
    pa.add_argument(
            '--expect-cells', required=False, default=3000,
            help='Number of expected cells in the library')

    args = pa.parse_args()

    print('Ready the pipeline')
    pipe = Pipeline(
            input_data_path=args.input_data,
            input_samplesheet_path=args.input_samplesheet,
            output_path=args.output,
            output_gdrive_path=args.output_gdrive,
            dataset_id=args.id,
            samplenames=args.samplenames,
            chemistry=args.chemistry,
            expect_cells=args.expect_cells,
            )

    print('Run the pipeline')
    pipe()
