#!/usr/bin/env python
'''
viscRNA-Seq Pipeline runner.

This script is to be used on user's machines to launch the viscRNA-Seq pipeline
on Amazon EC2 instances in the cloud. Although the script will by default wait
for the pipeline to finish, killing it after the initial launching period has
no consequence on the pipeline itself, which runs asynchronously.

This is an abstract script (!), so it cannot run per se because the pipeline
comes in different flavours, e.g. for different viruses. In order to create a
concrete instance of this script, the AMI_ID, GIT_COMMIT and
GIT_BRANCH global variables must be set to non-empty. The reason I
don't even allow environment variables or the likes is data consistency: only
scripts deployed from CI in a specific branch will and should be usable. No big
data crunching without testing.
'''
import os
import sys
import json
import time
import argparse
import subprocess as sp


AMI_ID = 'ami-be52d7c6'
GIT_BRANCH = ''
GIT_COMMIT = ''


class Runner:
    def broadcast_identity(self):
        print('viscRNA-Seq pipeline script: branch {:}, version {:}, AMI {:}'.format(
            GIT_BRANCH,
            GIT_COMMIT,
            AMI_ID,
            ))

    def get_pem_key(
            self,
            ):

        self.pemkey = 'viscrna-seq-key'
        self.pemkey_filename = 'key.pem'

        print('Copy SSH key from S3')
        src = 's3://viscrna-seq/assets/credentials/instance-keys/mCMV_10X.pem'
        try:
            sp.run(' '.join([
                'aws', 's3', 'cp',
                src, self.pemkey_filename,
                ]),
                shell=True,
                check=True)
        except sp.CalledProcessError as e:
            print(e.stdout)
            raise

        print('Restrict permissions on SSH key')
        os.chmod(self.pemkey_filename, 0o400)

        print('Done')
        print('NOTE: user on the remote machine is: ubuntu')

    def get_compatible_instance(
            self,
            ):
        try:
            out = sp.run(' '.join([
                'aws', 'ec2', 'describe-instances',
                '--filters', '"Name=image-id,Values={:}"'.format(AMI_ID),
                ]),
                shell=True,
                check=True,
                stdout=sp.PIPE).stdout
        except sp.CalledProcessError as e:
            print(e.stdout)
            raise

        outd = json.loads(out.decode())
        if 'Reservations' not in outd:
            return None

        for res in outd['Reservations']:
            if 'Instances' not in res:
                continue

            for instance in res['Instances']:
                instance_id = instance['InstanceId']
                state = instance['State']['Name']
                if state in ('stopped',):
                    self.instance_id = instance_id
                    return instance_id

        return None

    def get_remote_cmd(
            self,
            input_data_path,
            input_samplesheet_path,
            output_path,
            output_gdrive_path,
            dataset_id,
            samplenames,
            chemistry,
            expect_cells,
            ):
        remote_cmd = ' '.join([
            '/home/ubuntu/anaconda/bin/python',
            '/usr/local/bin/pipeline',
            '--input-data', input_data_path,
            '--input-samplesheet', input_samplesheet_path,
            '--output', output_path,
            '--output-gdrive', output_gdrive_path,
            '--id', dataset_id,
            '--samplenames'] + samplenames + [
            '--chemistry', chemistry,
            '--expect-cells', expect_cells,
            ])
        return remote_cmd

    def get_public_ip(self):
        try:
            out = sp.run(' '.join([
                'aws', 'ec2', 'describe-instances',
                '--instance-ids', self.instance_id,
                '--query', '"Reservations[*].Instances[*].PublicIpAddress"',
                '--output=text',
                ]),
                shell=True,
                check=True,
                stdout=sp.PIPE,
                ).stdout
        except sp.CalledProcessError as e:
            print(e.stdout)
            raise

        public_ip = out.decode().rstrip('\n')
        return public_ip

    def start_pipeline(
            self,
            input_data_path,
            input_samplesheet_path,
            output_path,
            output_gdrive_path,
            dataset_id,
            samplenames,
            chemistry,
            expect_cells,
            ):
        remote_cmd = self.get_remote_cmd(
                input_data_path=input_data_path,
                input_samplesheet_path=input_samplesheet_path,
                output_path=output_path,
                output_gdrive_path=output_gdrive_path,
                dataset_id=dataset_id,
                samplenames=samplenames,
                chemistry=chemistry,
                expect_cells=expect_cells,
                )

        public_ip = self.get_public_ip()

        try:
            call = ' '.join([
                'ssh',
                '-o', 'IdentitiesOnly=yes',
                '-o', '"StrictHostKeyChecking no"',
                '-i', self.pemkey_filename,
                'ubuntu@{:}'.format(public_ip),
                "'{:}'".format(remote_cmd),
                ])
            print(call)
            sp.run(
                call,
                shell=True,
                check=True,
                )
        except sp.CalledProcessError as e:
            self.stop_instance()
            print(e.stdout)
            raise

    def start_instance(
            self,
            dataset_id,
            ):

        if not hasattr(self, 'pemkey'):
            self.get_pem_key()

        try:
            out = sp.run(' '.join([
                'aws', 'ec2', 'create-tags',
                '--resources', self.instance_id,
                '--tags', "Key=datasetId,Value={:}".format(dataset_id),
                ]),
                shell=True,
                check=True,
                stdout=sp.PIPE).stdout
        except sp.CalledProcessError as e:
            print(e.stdout)
            raise

        try:
            out = sp.run(' '.join([
                'aws', 'ec2', 'start-instances',
                '--instance-ids', self.instance_id,
                ]),
                shell=True,
                check=True,
                stdout=sp.PIPE).stdout
        except sp.CalledProcessError as e:
            print(e.stdout)
            raise

        print('Started existing EC2 instance id: {:}'.format(instance_id))

    def launch_instance(
            self,
            dataset_id,
            ):
        if not hasattr(self, 'pemkey'):
            self.get_pem_key()

        tags = ','.join([
            '{Key=Name,Value=viscRNA-Seq-pipeline}',
            '{Key=Project,Value=viscRNA-Seq}',
            '{{Key=datasetId,Value={:}}}'.format(dataset_id),
            ])

        try:
            out = sp.run(' '.join([
                'aws', 'ec2', 'run-instances',
                '--image-id', AMI_ID,
                '--count', '1',
                '--instance-type', 'm4.2xlarge',
                '--instance-initiated-shutdown-behavior', 'terminate',
                '--tag-specifications',
                '"ResourceType=instance,Tags=[{:}]"'.format(tags),
                '"ResourceType=volume,Tags=[{:}]"'.format(tags),
                '--key-name', self.pemkey,
                ]),
                shell=True,
                check=True,
                stdout=sp.PIPE).stdout.decode()
        except sp.CalledProcessError as e:
            print(e.stdout)
            raise

        outd = json.loads(out)
        if ('Instances' not in outd) or (len(outd['Instances']) == 0):
            raise ValueError('No instances launched!')
        instance_id = outd['Instances'][0]['InstanceId']
        instance_state = outd['Instances'][0]['State']['Name']

        self.instance_id = instance_id
        print('Launched EC2 instance id: {:}, {:}'.format(instance_id, instance_state))
        return instance_id

    def wait_for_state(self, state, timeout=1000000):
        timeout = max(timeout, 0.05)
        t0 = time.time()
        t = t0
        while ((t - t0) / 60) <= timeout:
            try:
                out = sp.run(' '.join([
                    'aws', 'ec2', 'describe-instances',
                    '--instance-ids', self.instance_id,
                    '--query', '"Reservations[*].Instances[*].State.Name"',
                    '--output=text',
                    ]),
                    check=True,
                    shell=True,
                    stdout=sp.PIPE).stdout
            except sp.CalledProcessError as e:
                if e.stdout is not None:
                    print(e.stdout.decode())
                raise

            instance_state = out.decode().rstrip('\n')
            if instance_state == state:
                return instance_state

            if timeout >= 1:
                time.sleep(10)
                t = time.time()
                continue
            else:
                return instance_state

    def monitor_pipeline(self, timeout=40):
        console_output_buffer = []

        timeout = max(timeout, 0.05)
        t0 = time.time()
        t = t0
        while ((t - t0) / 60) <= timeout:
            try:
                out = sp.run(' '.join([
                    'aws', 'ec2', 'describe-instances',
                    '--instance-ids', self.instance_id,
                    '--query', '"Reservations[*].Instances[*].State.Name"',
                    '--output=text',
                    ]),
                    check=True,
                    shell=True,
                    stdout=sp.PIPE).stdout
            except sp.CalledProcessError as e:
                if e.stdout is not None:
                    print(e.stdout.decode())
                raise

            instance_state = out.decode().rstrip('\n')
            if instance_state != 'running':
                print('Job state: {:}'.format(instance_state))
                if timeout >= 1:
                    time.sleep(10)
                    t = time.time()
                    continue
                else:
                    break

            # Print console output
            try:
                out = sp.run(' '.join([
                    'aws', 'ec2', 'get-console-output',
                    '--instance-id', self.instance_id,
                    ]),
                    check=True,
                    shell=True,
                    stdout=sp.PIPE).stdout
            except sp.CalledProcessError as e:
                if e.stdout is not None:
                    print(e.stdout.decode())
                raise

            if out is None:
                raise ValueError('Output of get-console-output is None')

            lines = json.loads(out.decode())['Output'].split('\n')
            if len(lines) > 0:
                # Check in buffer
                for il, line in enumerate(console_output_buffer):
                    if line == lines[0]:
                        n_overlap = len(console_output_buffer) - il
                        if len(lines) <= n_overlap:
                            lines = []
                        else:
                            lines = lines[n_overlap:]
                        break

                for line in lines:
                    print(line)

                console_output_buffer.extend(lines)

                if len(console_output_buffer) > 50_000:
                    console_output_buffer = console_output_buffer[-50_000:]

            if timeout >= 1:
                time.sleep(10)
                t = time.time()
            else:
                break

    def stop_instance(self):
        sp.run(' '.join([
            'aws', 'ec2',
            'stop-instances',
            '--instance-ids', self.instance_id,
            ]),
            shell=True,
            check=True,
        )

    def terminate_instance(self):
        sp.run(' '.join([
            'aws', 'ec2',
            'terminate-instances',
            '--instance-ids', self.instance_id,
            ]),
            shell=True,
            check=True,
        )


if __name__ == '__main__':

    # FIXME: more specific help for the locations
    pa = argparse.ArgumentParser(description='''viscRNA-Seq pipeline''')
    spas = pa.add_subparsers(dest='subparser_name')

    par = spas.add_parser('run')
    par.add_argument(
            '--input-data', required=True,
            help='The location of the raw reads')
    par.add_argument(
            '--input-samplesheet', required=True,
            help='The location of the samplesheet')
    par.add_argument(
            '--output', required=True,
            help='The location of the output S3 bucket')
    par.add_argument(
            '--output-gdrive', required=False, default=None,
            help='The location on GoogleDrive where to store the count table')
    par.add_argument(
            '--id', required=True,
            help='Sets a unique ID for the dataset, for record keeping')
    par.add_argument(
            '--samplenames', nargs='+', required=True,
            help='Samplenames in the samplesheet')
    par.add_argument(
            '--chemistry', required=False, default='threeprime',
            choices=[
                'auto', 'threeprime', 'fiveprime',
                'SC3Pv1', 'SC3Pv2', 'SC5P-PE', 'SC5P-R2',
                ],
            help='Chemistry used in the data, see https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/count')
    par.add_argument(
            '--expect-cells', required=False, default=3000,
            help='Number of expected cells in the library')

    par.add_argument(
            '--monitor', type=int, default=None,
            help='Monitoring the pipeline after launch for X minutes (0 means once)')

    pam = spas.add_parser('monitor')
    pam.add_argument(
            '--instance-id', required=True,
            help='The id of the EC2 instance')
    pam.add_argument(
            '--timeout', type=int, default=0,
            help='Monitoring the pipeline after launch for X minutes (0 means once)')

    pas = spas.add_parser('terminate')
    pas.add_argument(
            '--instance-id', required=True,
            help='The id of the EC2 instance')

    args = pa.parse_args()

    run = Runner()
    run.broadcast_identity()

    if args.subparser_name == 'run':
        instance_id = run.get_compatible_instance()
        if instance_id is None:
            instance_id = run.launch_instance(
                    dataset_id=args.id,
                    )
        else:
            run.start_instance(
                    dataset_id=args.id,
                    )

        print('Waiting for instance to come online')
        run.wait_for_state('running', timeout=3)

        sys.exit()

        print('Start pipeline')
        run.start_pipeline(
                input_data_path=args.input_data,
                input_samplesheet_path=args.input_samplesheet,
                output_path=args.output,
                output_gdrive_path=args.output_gdrive,
                dataset_id=args.id,
                samplenames=args.samplenames,
                chemistry=args.chemistry,
                expect_cells=args.expect_cells,
            )

        if args.monitor is not None:
            print('Sleeping 1 minute before monitoring')
            time.sleep(60)
            print('Start monitoring')
            run.monitor_pipeline(timeout=args.monitor)

    elif args.subparser_name == 'monitor':
        run.instance_id = args.instance_id
        run.monitor_pipeline(timeout=args.timeout)

    elif args.subparser_name == 'terminate':
        run.instance_id = args.instance_id
        run.terminate_instance()

    else:
        raise ValueError('Subcommand not found: {:}'.format(args.subparser_name))
