#!/usr/bin/env python
'''
viscRNA-Seq Pipeline runner.

This script is to be used on user's machines to launch the viscRNA-Seq pipeline
on Amazon EC2 instances in the cloud. Although the script will by default wait
for the pipeline to finish, killing it after the initial launching period has
no consequence on the pipeline itself, which runs asynchronously.

This is an abstract script (!), so it cannot run per se because the pipeline
comes in different flavours, e.g. for different viruses. In order to create a
concrete instance of this script, the AMI_ID, GIT_COMMIT and
GIT_BRANCH global variables must be set to non-empty. The reason I
don't even allow environment variables or the likes is data consistency: only
scripts deployed from CI in a specific branch will and should be usable. No big
data crunching without testing.
'''
import os
import sys
import json
import time
import argparse
import subprocess as sp


AMI_ID = ''
GIT_BRANCH = ''
GIT_COMMIT = ''


class Runner:
    def launch_instance(self):
        #TODO
        ... AMI_ID...

        self.instance_id = instance_id
        print('EC2 instance id: {:}'.format(instance_id))
        return instance_id

    def terminate_instance(self, instance_id):
        sp.call(' '.join([
            'aws', 'ec2',
            #FIXME FIXME
            ... self.instance_id ... 
            '/usr/local/bin/pipeline',
            '--input-data', input_data_path,
            '--input-samplesheet', input_samplesheet_path,
            '--output', output_path,
            '--output-gdrive', output_gdrive_path,
            '--id', dataset_id,
            ]),
            shell=True,
            check=True,
        )

    def run_pipeline(
            self,
            input_path,
            output_path,
            output_gdrive_path,
            dataset_id,
            ):

        sp.call(' '.join([
            'aws', 'ec2',
            #FIXME FIXME
            ... self.instance_id ... 
            '/usr/bin/pipeline',
            '--input-data', input_data_path,
            '--input-samplesheet', input_samplesheet_path,
            '--output', output_path,
            '--output-gdrive', output_gdrive_path,
            '--id', dataset_id,
            ]),
            shell=True,
            check=True,
        )



if __name__ == '__main__':

    # FIXME: more specific help for the locations
    pa = argparse.ArgumentParser(description='''viscRNA-Seq pipeline''')
    spas = pa.add_subparsers()
    par = spas.add_parser('run')
    pa.add_argument(
            '--input-data', required=True,
            help='The location of the raw reads')
    pa.add_argument(
            '--input-samplesheet', required=True,
            help='The location of the samplesheet')
    par.add_argument(
            '--output', required=True,
            help='The location of the output S3 bucket')
    par.add_argument(
            '--output-gdrive', required=False, default=None,
            help='The location on GoogleDrive where to store the count table')
    par.add_argument(
            '--id', required=True,
            help='Sets a unique ID for the dataset, for record keeping')

    pas = spas.add_parser('stop')
    pas.add_argument(
            'instance-id',
            help='The id of the EC2 instance to stop')

    pa.parse_args()

    run = Runner()
    #TODO
    if ...
        instance_id = run.launch_instance()
        run.run_pipeline(
                input_data_path=args.input_data,
                input_samplesheet_path=args.input_samplesheet,
                output_path=args.output,
                output_gdrive_path=args.output_gdrive,
                dataset_id=args.id,
                )

    else:
        run.terminate_instance(args.instance_id)
